import torch
import numpy as np
import matplotlib.pyplot as plt
import logging
from scipy.special import huber
from torch import nn
import warnings
from scipy.optimize import minimize
from scipy.stats import linregress
from itertools import product
from tqdm import tqdm
import pickle

# Loss,è¿‡ç¨‹é‡ä¸ç»˜å›¾

def torch_huber(delta:float,r:torch.Tensor) -> torch.Tensor:
    '''åŸºäºtorchå¯¹huber_loss è¿›è¡Œè®¡ç®—'''
    return torch.where(torch.abs(r) < delta, 0.5 * r ** 2, delta * (torch.abs(r) - 0.5 * delta))

def huber_loss(r:np.ndarray,delta:float=0.001) -> np.ndarray:
    """Compute Huber loss for numpy array"""
    return huber(delta,r)

def preprocess_data(data:dict, file_names:list) -> dict:
    torch_data = {}
    for file_name in file_names:
        torch_data[file_name] = {
            "step": torch.tensor(data[file_name]["step"], dtype=torch.int32),
            "lrs": torch.tensor(data[file_name]["lrs"], dtype=torch.float64),
            "loss": torch.tensor(data[file_name]["loss"], dtype=torch.float32),
        }
        lr_sum = torch.cumsum(torch_data[file_name]["lrs"], dim=0, dtype=torch.float64)
        torch_data[file_name]["S1"] = lr_sum[torch_data[file_name]["step"]]
        torch_data[file_name]["lr_sum"] = lr_sum
        lr_gap = torch.zeros_like(torch_data[file_name]["lrs"])
        lr_gap[1:] = torch.diff(torch_data[file_name]["lrs"])
        torch_data[file_name]["lr_gap"] = lr_gap
    return torch_data

def compute_loss(model,torch_data, train_set, optimizer):
    """è®¡ç®—å…¨å±€æŸå¤±ï¼Œè¡¨ç°ä¼˜åŒ–æ­¥éª¤"""
    optimizer.zero_grad()
    total_loss = 0.0
    for file_name in train_set:
        args = [torch_data[file_name][key] for key in ["S1", "lrs", "lr_sum", "step", "lr_gap", "loss"]]
        total_loss += model(*args)
    total_loss.backward()
    optimizer.step()
    return total_loss

def compute_grad_norm(model):
    """è®¡ç®—æ¢¯åº¦çš„L2-èŒƒæ•°"""
    grads = [p.grad.flatten() for p in model.parameters() if p.grad is not None]
    return torch.cat(grads).norm() if grads else torch.tensor(0.0)

def log_step(step, total_loss, best_loss, model, grad_norm):
    """
    è®°å½•è®­ç»ƒè¿›åº¦å¹¶è¾“å‡ºå½“å‰æ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯ã€‚
    
    å‚æ•°è¯´æ˜ï¼š
        step (int)ï¼šå½“å‰è®­ç»ƒæ­¥éª¤ã€‚
        total_loss (float)ï¼šå½“å‰æ­¥éª¤çš„æŸå¤±å€¼ã€‚
        best_loss (float)ï¼šè¿„ä»Šä¸ºæ­¢è§‚å¯Ÿåˆ°çš„æœ€ä¼˜æŸå¤±å€¼ã€‚
        model (nn.Module)ï¼šMPL æ¨¡å‹å®ä¾‹ã€‚
        grad_norm (float)ï¼šå½“å‰æ­¥éª¤çš„æ¢¯åº¦èŒƒæ•°ã€‚
    """

    logger = logging.getLogger(__name__)
    params = {name: param.item() for name, param in model.named_parameters()}
    logger.info(f"Step {step:4d}: Loss={total_loss:.6f}, Best Loss={best_loss:.6f}, Grad Norm={grad_norm:.2e}")
    logger.info(f"Parameters: L0={params['L0']:.4f}, A={params['A']:.4f}, alpha={params['alpha']:.4f}, "
                f"B={params['B']:.4f}, C={params['C']:.4f}, beta={params['beta']:.4f}, gamma={params['beta']:.4f}")

def plot_loss_curve(loss_history, fig_folder):
    """Plot and save loss curve."""
    plt.figure(figsize=(8, 6))
    plt.plot(np.arange(len(loss_history)), loss_history, label="Fitting Loss")
    plt.xlabel("Step")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    plt.savefig(f"{fig_folder}/loss_monitor.png")
    plt.close()

class MPL(nn.Module):
    """
    åŸºäºå­¦ä¹ ç‡è°ƒåº¦é¢„æµ‹è®­ç»ƒæŸå¤±çš„å¤šé‡å¹‚å¾‹ï¼ˆMPLï¼‰æ¨¡å‹ï¼ˆå¼ºåˆ¶è®¾å®š gamma = betaï¼‰ã€‚

    å‚æ•°è¯´æ˜ï¼š
        L0 (float): åŸºçº¿æŸå¤±å‚æ•°ã€‚
        A (float): å¹‚å¾‹è¡°å‡é¡¹çš„å¹…å€¼ã€‚
        alpha (float): å¹‚å¾‹è¡°å‡é¡¹çš„æŒ‡æ•°ã€‚
        B (float): æŸå¤±çªé™é¡¹çš„å¹…å€¼ã€‚
        C (float): æŸå¤±çªé™å˜æ¢ä¸­çš„ç¼©æ”¾å› å­ã€‚
        beta (float): å¹‚å¾‹çªé™é¡¹ä¸­çš„æŒ‡æ•°ï¼ˆåŒæ—¶ç”¨äº lrs çš„æŒ‡æ•°ï¼Œå³ gammaï¼‰ã€‚
    """

    def __init__(self, L0, A, alpha, B, C, beta):
        super().__init__()
        self.L0 = nn.Parameter(torch.tensor(L0, dtype=torch.float64))
        self.A = nn.Parameter(torch.tensor(A, dtype=torch.float64))
        self.alpha = nn.Parameter(torch.tensor(alpha, dtype=torch.float64))
        self.B = nn.Parameter(torch.tensor(B, dtype=torch.float64))
        self.C = nn.Parameter(torch.tensor(C, dtype=torch.float64))
        self.beta = nn.Parameter(torch.tensor(beta, dtype=torch.float64))
        # gamma è¢«çœç•¥ï¼Œæ¨¡å‹å†…éƒ¨ç»Ÿä¸€ç”¨ beta ä»£æ›¿

    def forward(self, S1, lrs, lr_sum, step, lr_gap, loss):
        LD = torch.zeros_like(step, dtype=torch.float64)
        for i, s in enumerate(step):
            if s > 0:
                LD[i] = torch.sum(
                    lr_gap[1:s+1] * (
                        1 - (1 + self.C * lrs[1:s+1] ** (-self.beta) * (lr_sum[s] - lr_sum[:s])) ** (-self.beta)
                    )
                )
        pred = self.L0 + self.A * S1 ** (-self.alpha) + self.B * LD
        r = torch.log(loss) - torch.log(pred.clamp(min=1e-10))
        return torch_huber(0.001, r).sum()



from src.config import  FIT_MAX_STEPS,FIT_EVAL_INTERVAL, FIT_LR1, FIT_LR2, FIT_GRAD_NORM_THR, FIT_LOSS_THR, FIT_PATIENCE

def initialize_params(data: dict, train_set: list) -> list:
    """
    ä½¿ç”¨ç½‘æ ¼æœç´¢å’Œ L-BFGS-B ä¼˜åŒ–å™¨åˆå§‹åŒ– MPL æ¨¡å‹çš„å‚æ•°ã€‚

    å‚æ•°ï¼š
        data (dict)ï¼šåŒ…å«æ¯ä¸ªæ–‡ä»¶çš„æ­¥éª¤ã€å­¦ä¹ ç‡å’ŒæŸå¤±å€¼çš„æ•°æ®é›†ã€‚
        train_set (list)ï¼šè®­ç»ƒæ–‡ä»¶ååˆ—è¡¨ã€‚

    è¿”å›å€¼ï¼š
        listï¼šåˆå§‹å‚æ•°ä¼°è®¡å€¼ [L0, A, alpha, B]ã€‚
    """
    logger = logging.getLogger(__name__)
    logger.info("å¼€å§‹å‚æ•°åˆå§‹åŒ–")

    # è®¡ç®—è®­ç»ƒé›†ä¸­æ‰€æœ‰æ–‡ä»¶çš„æœ€å°æŸå¤±å€¼
    min_loss = min(data[file_name]["loss"].min() for file_name in train_set)
    log_y_list, log_x_list = [], []

    # æ„é€  log-log å›å½’æ‰€éœ€çš„æ•°æ®
    for file_name in train_set:
        log_y = np.log(data[file_name]["loss"] - min_loss + 0.01)
        log_x = np.log(np.cumsum(data[file_name]["lrs"])[data[file_name]["step"]])
        log_y_list.append(log_y)
        log_x_list.append(log_x)

    log_y = np.concatenate(log_y_list)
    log_x = np.concatenate(log_x_list)
    slope, intercept, _, _, _ = linregress(log_x, log_y)

    # åˆå§‹å‚æ•°ç½‘æ ¼è®¾å®š
    L0_init_set = np.linspace(min_loss - 0.2, min_loss + 0.2, 5)
    A_init_set = np.linspace(np.exp(intercept) - 0.1, np.exp(intercept) + 0.1, 3)
    alpha_init_set = np.linspace(-slope - 0.1, -slope + 0.1, 3)
    B_init_set = np.linspace(100, 1000, 3)

    # æ‹Ÿåˆçš„ç›®æ ‡æŸå¤±å‡½æ•°ï¼ˆHuber Lossï¼‰
    def loss_fn0(params):
        L0, A, alpha, B = params
        total_loss = 0
        for file_name in train_set:
            lr = data[file_name]["lrs"]
            step = data[file_name]["step"]
            pred = L0 + A * np.cumsum(lr)[step] ** (-alpha) - B * (3e-4 - lr[step])
            loss = data[file_name]["loss"]
            r = np.log(loss) - np.log(pred)
            total_loss += huber_loss(r).sum()
        return total_loss

    init_params = list(product(L0_init_set, A_init_set, alpha_init_set, B_init_set))
    best_loss = float('inf')
    best_params = None

    # éå†æ‰€æœ‰åˆå§‹å‚æ•°ç»„åˆï¼Œå¯»æ‰¾æœ€ä¼˜è§£
    for init_param in tqdm(init_params, desc="åˆå§‹åŒ–å‚æ•°æœç´¢"):
        res = minimize(
            loss_fn0, init_param, method='L-BFGS-B', bounds=[(0, np.inf)] * 4,
            options={'maxiter': 100000, 'ftol': 1e-9, 'gtol': 1e-6, 'eps': 1e-8}
        )
        if res.fun < best_loss:
            best_loss = res.fun
            best_params = res.x

    logger.info(f"åˆå§‹åŒ–å®Œæˆã€‚æœ€ä¼˜æŸå¤±: {best_loss}, æœ€ä¼˜å‚æ•°: {best_params}")
    return best_params
def generate_init_params(init_param: list) -> list:
    """
    åŸºäºåˆå§‹å‚æ•°ç”Ÿæˆ MPL å‚æ•°ç»„åˆï¼ˆçº¦æŸ beta = gammaï¼‰ã€‚

    å‚æ•°ï¼š
        init_param (list): [L0, A, alpha, B]

    è¿”å›å€¼ï¼š
        list: æ¯ç»„ä¸º [L0, A, alpha, B, C, beta]
    """
    L0, A, alpha, B = init_param
    init_C_param = [1.0]
    init_beta_param = [0.5]
    return list(product([L0], [A], [alpha], [B], init_C_param, init_beta_param))


def mpl_adam_fit(
    data,
    train_set,
    test_set,
    init_params,
    fig_folder,
    eval_interval=FIT_EVAL_INTERVAL,
    lr1=FIT_LR1,
    lr2=FIT_LR2,
    max_steps=FIT_MAX_STEPS,
    grad_norm_thr=FIT_GRAD_NORM_THR,
    loss_thr=FIT_LOSS_THR,
    patience=FIT_PATIENCE
):
    """
    ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨å¯¹ MPL æ¨¡å‹è¿›è¡Œæ‹Ÿåˆã€‚

    å‚æ•°ï¼š
        data (dict)ï¼šåŒ…å«æ­¥éª¤ã€å­¦ä¹ ç‡å’ŒæŸå¤±çš„æ•°æ®é›†ã€‚
        train_set (list)ï¼šè®­ç»ƒæ–‡ä»¶ååˆ—è¡¨ã€‚
        test_set (list)ï¼šæµ‹è¯•æ–‡ä»¶ååˆ—è¡¨ã€‚
        init_params (list)ï¼šMPL æ¨¡å‹çš„åˆå§‹å‚æ•°ç»„ã€‚
        fig_folder (str)ï¼šä¿å­˜æŸå¤±æ›²çº¿çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
        eval_interval (int)ï¼šè®°å½•è¯„ä¼°çš„é—´éš”æ­¥æ•°ï¼ˆé»˜è®¤æ¥è‡ª configï¼‰ã€‚
        lr1 (float)ï¼šç”¨äº L0ã€Aã€Bã€C å‚æ•°çš„å­¦ä¹ ç‡ï¼ˆé»˜è®¤æ¥è‡ª configï¼‰ã€‚
        lr2 (float)ï¼šç”¨äº alphaã€betaã€gamma å‚æ•°çš„å­¦ä¹ ç‡ï¼ˆé»˜è®¤æ¥è‡ª configï¼‰ã€‚
        max_steps (int)ï¼šæœ€å¤§è®­ç»ƒæ­¥æ•°ï¼ˆé»˜è®¤æ¥è‡ª configï¼‰ã€‚
        grad_norm_thr (float)ï¼šæ¢¯åº¦èŒƒæ•°çš„åœæ­¢é˜ˆå€¼ï¼ˆé»˜è®¤æ¥è‡ª configï¼‰ã€‚
        loss_thr (float)ï¼šæŸå¤±æ”¹è¿›çš„åœæ­¢é˜ˆå€¼ï¼ˆé»˜è®¤æ¥è‡ª configï¼‰ã€‚
        patience (int)ï¼šåœ¨æ— æ”¹è¿›çš„æƒ…å†µä¸‹ç»§ç»­è®­ç»ƒçš„æ­¥æ•°ï¼ˆé»˜è®¤æ¥è‡ª configï¼‰ã€‚

    è¿”å›å€¼ï¼š
        tupleï¼šæœ€ä¼˜å‚æ•°ç»„å’Œå¯¹åº”çš„æœ€å°æŸå¤±å€¼ã€‚
    """
    logger = logging.getLogger(__name__)
    logger.info("å¼€å§‹ä½¿ç”¨ AdamW è¿›è¡Œ MPL æ‹Ÿåˆ")

    torch_data = preprocess_data(data, train_set + test_set)
    best_params, best_loss = None, float('inf')

    # éå†æ‰€æœ‰åˆå§‹å‚æ•°ç»„ï¼Œè¿›è¡Œè®­ç»ƒ
    for init_param in init_params:
        logger.info(f"Initializing with parameters: {init_param}")
        model = MPL(*init_param)  # unpack 6 params
    
        optimizer = torch.optim.AdamW([
            {"params": [model.L0, model.A, model.B, model.C], "lr": lr1},
            {"params": [model.alpha, model.beta], "lr": lr2},  # gamma è¢«ç§»é™¤
        ])


        loss_history, min_loss, steps_no_improve = [], float('inf'), 0

        for step in tqdm(range(max_steps), desc="è®­ç»ƒè¿›åº¦"):
            total_loss = compute_loss(model, torch_data, train_set, optimizer)
            loss_history.append(total_loss.item())

            if total_loss < min_loss - loss_thr:
                min_loss = total_loss.item()
                steps_no_improve = 0
            else:
                steps_no_improve += 1

            if step > patience and steps_no_improve >= patience:
                logger.info(f"æå‰åœæ­¢äº step {step}ï¼š{patience} æ­¥æ— æ”¹è¿›ã€‚")
                break

            grad_norm = compute_grad_norm(model)
            if grad_norm < grad_norm_thr:
                logger.info(f"æå‰åœæ­¢äº step {step}ï¼šæ¢¯åº¦èŒƒæ•° {grad_norm:.2e} å°äºé˜ˆå€¼ {grad_norm_thr:.2e}")
                break

            if total_loss < best_loss:
                best_loss = total_loss.item()
                best_params = [p.item() for p in model.parameters()]
                logger.info(f"å‘ç°æ›´ä¼˜æŸå¤±ï¼š{best_loss}")

            if step % eval_interval == 0:
                log_step(step, total_loss, best_loss, model, grad_norm)

        plot_loss_curve(loss_history, fig_folder)

    logger.info(f"æ‹Ÿåˆå®Œæˆã€‚æœ€ä¼˜æŸå¤±: {best_loss}, æœ€ä¼˜å‚æ•°: {best_params}")
    return best_params, best_loss

with open(r"C:\Users\lenovo\Desktop\processed_data_50_after2500.pkl", "rb") as f:
    data = pickle.load(f)

results = {}

# æ¯ä¸ªæ ·æœ¬ä½œä¸ºè‡ªå·±çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†
for name in data:
    train_set = [name]
    test_set = [name]

    init_param = initialize_params(data, train_set)
    init_params = generate_init_params(init_param)

    best_params, best_loss = mpl_adam_fit(
        data=data,
        train_set=train_set,
        test_set=test_set,
        init_params=init_params,
        fig_folder="results",
        eval_interval=FIT_EVAL_INTERVAL,
        lr1=FIT_LR1,
        lr2=FIT_LR2,
        max_steps=1000,
        grad_norm_thr=FIT_GRAD_NORM_THR,
        loss_thr=FIT_LOSS_THR,
        patience=80
    )

    results[name] = {
        "best_params": best_params,
        "best_loss": best_loss
    }

# æ‰“å°ç»“æœ
for k, v in results.items():
    print(f"\nğŸ“˜ {k}")
    print("âœ… æœ€ä¼˜å‚æ•°:", v["best_params"])
    print(f"âœ… æœ€å°æŸå¤±: {v['best_loss']:.6f}")
